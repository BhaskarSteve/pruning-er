# sft.yaml  (fixed for single A40)
base_model: Qwen/Qwen2.5-0.5B-Instruct
trust_remote_code: true

datasets:
  - path: data/s1k-gemini_chatml.jsonl
    type: chat_template
    chat_template: chatml
    field_messages: messages

# Do NOT learn from the user turn
train_on_inputs: false

# ==== Training schedule ====
num_train_epochs: 3

# Use micro-batching; keep effective batch ~=16 like the paper
micro_batch_size: 1            # fits A40 safely; try 2 if VRAM is comfy
gradient_accumulation_steps: 16
batch_size: 16                 # (derived; redundant but silences validator)

optimizer: adamw_hf
learning_rate: 1.0e-5
weight_decay: 1.0e-4
adam_beta1: 0.9
adam_beta2: 0.95

lr_scheduler_type: cosine
warmup_ratio: 0.05

bf16: true
tf32: true
gradient_checkpointing: true   # big memory saver

# ==== Context length ====
# 32k is great but usually OOM on a single A40. Start with 8k; if it fits, try 16k.
sequence_len: 8192
sample_packing: false

# Misc
save_strategy: "epoch"
evaluation_strategy: "none"
logging_steps: 10
seed: 42
output_dir: outputs/qwen25-3b-s1k-sft
